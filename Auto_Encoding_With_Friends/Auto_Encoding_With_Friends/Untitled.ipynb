{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Auto-Encoders are a form of dimensionality reduction in the data science field.  Much like Principle Component Analysis, they aim to reduce the complexity of a model so as to not over fit.  An example can be seen here where we have a very over fit model vs. an under fit model.  There clearly is a sweet spot between these two models that would present a better fit model/best fit model but for this learning exercise we will keep it simple.\n",
    "So at the end of the day what AE wants to accomplish is creating a model with less dimensions so that it can be reproduced over a new set of data (test set.. same as training set) and still be accurate.  If the model on the left saw a new set of data, it would be highly unlikely for it to be accurate. \n",
    "\n",
    "![](pic1.png)\n",
    "\n",
    "\n",
    "**So, here is a general outline of Auto Encoders:**\n",
    "\n",
    "•\t*It is a form of a machine learning model*\n",
    "\n",
    "•\t*It is part of the neural network family*\n",
    "\n",
    "•\t*It is a unsupervised model*\n",
    "\n",
    "•\t*The output should match the input*\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The output matching the input is what separates AE from other dimensional reducing machine learning algorithms.  Here is a photo of what a typical AE network looks like.  \n",
    "\n",
    "![](pic2.png)\n",
    "\n",
    "Notice that the input (xn) matches the output (x’n). \n",
    "\n",
    "\n",
    "The hidden layer inside the neural network has activation functions that allow it to encode nonlinearities.\n",
    "\n",
    "![](pic3.png)\n",
    "\n",
    "\n",
    "\n",
    "So where a normal model that would want to predict an outcome would present code such as: \n",
    "\n",
    "model.fit(X, y) \n",
    "\n",
    "\n",
    "\n",
    "**The AE would have code:**\n",
    "\n",
    "**model.fit(X, X)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Very Simple Example\n",
    "\n",
    "Let’s say we want to Auto Encode an array ([1, 0, 0, 1, 1]).  \n",
    "\n",
    "An Auto Encoder would want to output the same array  ([1, 0, 0, 1, 1]).  --> xy = xy\n",
    "\n",
    "\n",
    "\n",
    "So we input these 5 dimensions and say we use 3 neurons in the hidden layer (just like the picture above).  \n",
    "\n",
    "Using activation functions, the neural network would try to reproduce that same array.  If we were to train the neural network with thousands of data points, these neurons would aim to find the weights that minimize the reconstruction error.  Those same weights are what is used to transform the array ([1, 0, 0, 1, 1]) into the output of ([1, 0, 0, 1, 1]).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
